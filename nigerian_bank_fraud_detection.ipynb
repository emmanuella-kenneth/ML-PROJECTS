{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¦ Nigerian Bank Transaction Fraud Detection\n",
    "## End-to-End Data Science Project\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Data Science Portfolio Project  \n",
    "**Domain:** Financial Technology (FinTech) | Fraud Analytics  \n",
    "**Tools:** Python, Pandas, Scikit-learn, Matplotlib, Seaborn\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Project Overview\n",
    "\n",
    "Financial fraud is a critical challenge for Nigerian banks. According to the Financial Institutions Training Centre (FITC), Nigerian banks lost over **â‚¦9.5 billion** to fraud in 2023 alone. This project builds a **Machine Learning-powered fraud detection system** that can automatically flag suspicious transactions in real-time, helping banks like GTBank, Access Bank, Zenith Bank, and UBA protect their customers.\n",
    "\n",
    "## ğŸ¯ Business Objectives\n",
    "\n",
    "1. **Detect** fraudulent transactions with high accuracy before they are processed\n",
    "2. **Reduce** financial losses to the bank and customers\n",
    "3. **Minimize** false positives (blocking genuine customers)\n",
    "4. **Provide** interpretable insights for the fraud investigation team\n",
    "5. **Quantify** the business impact of deploying the model\n",
    "\n",
    "## ğŸ“‹ Project Workflow\n",
    "\n",
    "```\n",
    "1. Data Simulation       â†’ Generate realistic Nigerian bank transactions\n",
    "2. Exploratory Analysis  â†’ Understand patterns, distributions, fraud indicators\n",
    "3. Data Cleaning         â†’ Handle missing values, outliers, feature engineering\n",
    "4. Anomaly Detection     â†’ Statistical methods to flag suspicious transactions\n",
    "5. ML Classification     â†’ Train & evaluate fraud detection models\n",
    "6. Business Impact       â†’ Translate model performance into â‚¦ savings\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 1: Environment Setup & Library Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================================\n",
    "\n",
    "# Core data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, average_precision_score,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "COLORS = {'fraud': '#E74C3C', 'legit': '#2ECC71', 'neutral': '#3498DB', 'accent': '#F39C12'}\n",
    "\n",
    "print('âœ… All libraries imported successfully!')\n",
    "print(f'   NumPy version  : {np.__version__}')\n",
    "print(f'   Pandas version : {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 2: Data Simulation\n",
    "### Generating Realistic Nigerian Bank Transaction Data\n",
    "---\n",
    "\n",
    "Since real bank data is confidential, we simulate transactions using domain knowledge of Nigerian banking patterns:\n",
    "- **NIBSS** (Nigeria Inter-Bank Settlement System) transaction types\n",
    "- Typical Nigerian salary ranges and spending patterns\n",
    "- Common fraud vectors: ATM skimming, POS fraud, mobile banking takeover, BVN fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA SIMULATION: Nigerian Bank Transactions\n",
    "# ============================================================\n",
    "\n",
    "def simulate_nigerian_bank_data(n_transactions=50000, fraud_rate=0.025):\n",
    "    \"\"\"\n",
    "    Simulates realistic Nigerian bank transaction data.\n",
    "    \n",
    "    Fraud rate ~2.5% reflects industry estimates from FITC Nigeria reports.\n",
    "    Features are designed to mirror real NIBSS transaction attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_fraud = int(n_transactions * fraud_rate)\n",
    "    n_legit = n_transactions - n_fraud\n",
    "    \n",
    "    # --- Nigerian Geographic & Banking Context ---\n",
    "    states = ['Lagos', 'Abuja', 'Rivers', 'Kano', 'Oyo', 'Delta', 'Anambra',\n",
    "              'Kaduna', 'Ogun', 'Edo', 'Imo', 'Enugu', 'Cross River', 'Plateau']\n",
    "    \n",
    "    channels = ['Mobile App', 'ATM', 'POS', 'Internet Banking', 'USSD', 'Branch']\n",
    "    channel_weights = [0.35, 0.20, 0.22, 0.12, 0.08, 0.03]  # Mobile is dominant in Nigeria\n",
    "    \n",
    "    transaction_types = ['Transfer', 'Withdrawal', 'Payment', 'Airtime', 'Utility Bill',\n",
    "                         'Salary Credit', 'Card Purchase', 'International Transfer']\n",
    "    \n",
    "    merchant_categories = ['Supermarket', 'Fuel Station', 'Restaurant', 'Hospital',\n",
    "                            'Telecom', 'Utilities', 'Electronics', 'Fashion', \n",
    "                            'Betting', 'Online Merchant', 'Money Transfer', 'ATM']\n",
    "    \n",
    "    # ============================================================\n",
    "    # GENERATE LEGITIMATE TRANSACTIONS\n",
    "    # ============================================================\n",
    "    \n",
    "    # Transaction timestamps â€” business hours weighted higher\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    legit_raw = np.array([1,1,1,1,1,2,3,6,7,8,7,7,7,7,7,6,5,5,4,4,3,3,2,1], dtype=float)\n",
    "    legit_hour_p = legit_raw / legit_raw.sum()\n",
    "    legit_hours = np.random.choice(\n",
    "        range(24),\n",
    "        size=n_legit,\n",
    "        p=legit_hour_p\n",
    "    )\n",
    "    legit_days = np.random.randint(0, 365, n_legit)\n",
    "    \n",
    "    # Transaction amounts â€” Nigerian Naira, log-normal distribution\n",
    "    # Average Nigerian salary ~â‚¦100,000â€“â‚¦400,000/month\n",
    "    legit_amounts = np.random.lognormal(mean=10.5, sigma=1.8, size=n_legit)\n",
    "    legit_amounts = np.clip(legit_amounts, 100, 5_000_000)  # â‚¦100 to â‚¦5M\n",
    "    \n",
    "    # Customer profiles\n",
    "    legit_account_age_days = np.random.gamma(shape=5, scale=200, size=n_legit)  # avg 1000 days\n",
    "    legit_avg_balance = np.random.lognormal(mean=11.5, sigma=1.5, size=n_legit)\n",
    "    legit_txn_count_30d = np.random.poisson(lam=20, size=n_legit)  # avg 20 txns/month\n",
    "    legit_txn_count_7d = np.random.poisson(lam=5, size=n_legit)\n",
    "    legit_distance_from_home = np.random.exponential(scale=15, size=n_legit)  # km\n",
    "    legit_failed_attempts = np.random.choice([0, 1, 2], size=n_legit, p=[0.92, 0.06, 0.02])\n",
    "    legit_is_new_recipient = np.random.choice([0, 1], size=n_legit, p=[0.78, 0.22])\n",
    "    \n",
    "    # ============================================================\n",
    "    # GENERATE FRAUDULENT TRANSACTIONS\n",
    "    # ============================================================\n",
    "    \n",
    "    # Fraudsters prefer late night / early morning\n",
    "    fraud_raw = np.array([5,6,7,7,6,5,4,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5], dtype=float)\n",
    "    fraud_hour_p = fraud_raw / fraud_raw.sum()\n",
    "    fraud_hours = np.random.choice(\n",
    "        range(24),\n",
    "        size=n_fraud,\n",
    "        p=fraud_hour_p\n",
    "    )\n",
    "    fraud_days = np.random.randint(0, 365, n_fraud)\n",
    "    \n",
    "    # Fraud amounts â€” either very small (testing) or very large (drain)\n",
    "    fraud_amount_type = np.random.choice(['small', 'large'], size=n_fraud, p=[0.3, 0.7])\n",
    "    fraud_amounts = np.where(\n",
    "        fraud_amount_type == 'small',\n",
    "        np.random.uniform(50, 1000, n_fraud),          # Micro-probing transactions\n",
    "        np.random.lognormal(mean=13, sigma=1.2, size=n_fraud)  # Large draining\n",
    "    )\n",
    "    fraud_amounts = np.clip(fraud_amounts, 50, 50_000_000)\n",
    "    \n",
    "    # Fraudulent account characteristics\n",
    "    fraud_account_age_days = np.random.gamma(shape=1.5, scale=100, size=n_fraud)  # Newer accounts\n",
    "    fraud_avg_balance = np.random.lognormal(mean=10, sigma=1.2, size=n_fraud)\n",
    "    fraud_txn_count_30d = np.random.poisson(lam=40, size=n_fraud)   # Unusual spike\n",
    "    fraud_txn_count_7d = np.random.poisson(lam=12, size=n_fraud)    # High recent activity\n",
    "    fraud_distance_from_home = np.random.exponential(scale=200, size=n_fraud)  # Far from home\n",
    "    fraud_failed_attempts = np.random.choice([0, 1, 2, 3], size=n_fraud, p=[0.40, 0.25, 0.20, 0.15])\n",
    "    fraud_is_new_recipient = np.random.choice([0, 1], size=n_fraud, p=[0.20, 0.80])  # New recipients\n",
    "    \n",
    "    # ============================================================\n",
    "    # COMBINE & BUILD DATAFRAME\n",
    "    # ============================================================\n",
    "    \n",
    "    all_hours   = np.concatenate([legit_hours, fraud_hours])\n",
    "    all_days    = np.concatenate([legit_days, fraud_days])\n",
    "    all_amounts = np.concatenate([legit_amounts, fraud_amounts])\n",
    "    all_labels  = np.concatenate([np.zeros(n_legit), np.ones(n_fraud)])\n",
    "    \n",
    "    # Shuffle everything together\n",
    "    shuffle_idx = np.random.permutation(n_transactions)\n",
    "    \n",
    "    all_accounts = [f'NG{np.random.randint(10_000_000, 99_999_999)}' for _ in range(n_transactions)]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'transaction_id'       : [f'TXN{i:08d}' for i in range(n_transactions)],\n",
    "        'account_number'       : all_accounts,\n",
    "        'transaction_date'     : [start_date + timedelta(days=int(d)) for d in all_days[shuffle_idx]],\n",
    "        'transaction_hour'     : all_hours[shuffle_idx],\n",
    "        'amount_ngn'           : np.round(all_amounts[shuffle_idx], 2),\n",
    "        'transaction_type'     : np.random.choice(transaction_types, n_transactions),\n",
    "        'channel'              : np.random.choice(channels, n_transactions, p=channel_weights),\n",
    "        'merchant_category'    : np.random.choice(merchant_categories, n_transactions),\n",
    "        'state'                : np.random.choice(states, n_transactions),\n",
    "        'account_age_days'     : np.concatenate([\n",
    "                                    legit_account_age_days, fraud_account_age_days\n",
    "                                 ])[shuffle_idx].round(0),\n",
    "        'avg_balance_ngn'      : np.concatenate([\n",
    "                                    legit_avg_balance, fraud_avg_balance\n",
    "                                 ])[shuffle_idx].round(2),\n",
    "        'txn_count_last_30d'   : np.concatenate([\n",
    "                                    legit_txn_count_30d, fraud_txn_count_30d\n",
    "                                 ])[shuffle_idx],\n",
    "        'txn_count_last_7d'    : np.concatenate([\n",
    "                                    legit_txn_count_7d, fraud_txn_count_7d\n",
    "                                 ])[shuffle_idx],\n",
    "        'distance_from_home_km': np.concatenate([\n",
    "                                    legit_distance_from_home, fraud_distance_from_home\n",
    "                                 ])[shuffle_idx].round(2),\n",
    "        'failed_attempts_24h'  : np.concatenate([\n",
    "                                    legit_failed_attempts, fraud_failed_attempts\n",
    "                                 ])[shuffle_idx],\n",
    "        'is_new_recipient'     : np.concatenate([\n",
    "                                    legit_is_new_recipient, fraud_is_new_recipient\n",
    "                                 ])[shuffle_idx],\n",
    "        'is_fraud'             : all_labels[shuffle_idx].astype(int)\n",
    "    })\n",
    "    \n",
    "    # Inject ~3% missing values in non-critical fields (realistic data quality)\n",
    "    for col in ['merchant_category', 'distance_from_home_km', 'avg_balance_ngn']:\n",
    "        missing_mask = np.random.choice([True, False], size=len(df), p=[0.03, 0.97])\n",
    "        df.loc[missing_mask, col] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Generate the dataset\n",
    "df_raw = simulate_nigerian_bank_data(n_transactions=50000, fraud_rate=0.025)\n",
    "\n",
    "print('=' * 60)\n",
    "print('  âœ… DATASET GENERATED SUCCESSFULLY')\n",
    "print('=' * 60)\n",
    "print(f'  Total Transactions : {len(df_raw):,}')\n",
    "print(f'  Fraudulent         : {df_raw[\"is_fraud\"].sum():,} ({df_raw[\"is_fraud\"].mean()*100:.1f}%)')\n",
    "print(f'  Legitimate         : {(df_raw[\"is_fraud\"]==0).sum():,} ({(df_raw[\"is_fraud\"]==0).mean()*100:.1f}%)')\n",
    "print(f'  Features           : {df_raw.shape[1] - 1}')\n",
    "print(f'  Date Range         : {df_raw[\"transaction_date\"].min().date()} to {df_raw[\"transaction_date\"].max().date()}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first few rows\n",
    "print('ğŸ“‹ DATASET PREVIEW:')\n",
    "df_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset structure and data types\n",
    "print('ğŸ—‚ï¸  DATASET INFO:')\n",
    "df_raw.info()\n",
    "print('\\nğŸ“Š STATISTICAL SUMMARY:')\n",
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 3: Exploratory Data Analysis (EDA)\n",
    "### Understanding Patterns, Distributions & Fraud Signals\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.1 â€” CLASS DISTRIBUTION (Fraud vs Legitimate)\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "labels = ['Legitimate', 'Fraudulent']\n",
    "counts = df_raw['is_fraud'].value_counts().sort_index()\n",
    "bars = axes[0].bar(labels, counts, color=[COLORS['legit'], COLORS['fraud']], \n",
    "                   edgecolor='white', linewidth=2, width=0.5)\n",
    "axes[0].set_title('Transaction Class Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "axes[0].set_ylabel('Number of Transactions', fontsize=12)\n",
    "for bar, count in zip(bars, counts):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 200,\n",
    "                f'{count:,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "axes[0].yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, p: f'{x:,.0f}'))\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(counts, labels=labels, colors=[COLORS['legit'], COLORS['fraud']],\n",
    "            autopct='%1.1f%%', startangle=90, textprops={'fontsize': 12},\n",
    "            wedgeprops={'edgecolor': 'white', 'linewidth': 2})\n",
    "axes[1].set_title('Fraud Rate Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "plt.suptitle('âš ï¸ Dataset is Imbalanced â€” A Key Challenge in Fraud Detection', \n",
    "             fontsize=13, y=1.02, color='gray', style='italic')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ’¡ KEY INSIGHT: Class Imbalance\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Only {df_raw['is_fraud'].mean()*100:.1f}% of transactions are fraud. This is realistic but\n",
    "creates a challenge: a naive model that labels EVERYTHING as\n",
    "legitimate would achieve {(1-df_raw['is_fraud'].mean())*100:.1f}% accuracy â€” but catch ZERO fraud!\n",
    "\n",
    "Strategy: Use class_weight='balanced' in models + prioritize\n",
    "Recall and ROC-AUC over simple accuracy.\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.2 â€” TRANSACTION AMOUNT ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "fraud_amounts  = df_raw[df_raw['is_fraud'] == 1]['amount_ngn']\n",
    "legit_amounts  = df_raw[df_raw['is_fraud'] == 0]['amount_ngn']\n",
    "\n",
    "# Log-scale distribution\n",
    "axes[0].hist(np.log10(legit_amounts + 1), bins=60, alpha=0.7,\n",
    "             color=COLORS['legit'], label=f'Legitimate (n={len(legit_amounts):,})', density=True)\n",
    "axes[0].hist(np.log10(fraud_amounts + 1), bins=60, alpha=0.7,\n",
    "             color=COLORS['fraud'], label=f'Fraud (n={len(fraud_amounts):,})', density=True)\n",
    "axes[0].set_title('Transaction Amount Distribution (Logâ‚â‚€ Scale)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Logâ‚â‚€(Amount in â‚¦)', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Nigerian Naira x-tick labels\n",
    "tick_values = [2, 3, 4, 5, 6, 7]\n",
    "tick_labels = ['â‚¦100', 'â‚¦1K', 'â‚¦10K', 'â‚¦100K', 'â‚¦1M', 'â‚¦10M']\n",
    "axes[0].set_xticks(tick_values)\n",
    "axes[0].set_xticklabels(tick_labels, rotation=30)\n",
    "\n",
    "# Box plot comparison\n",
    "amount_data = [\n",
    "    np.log10(legit_amounts.clip(lower=1) + 1),\n",
    "    np.log10(fraud_amounts.clip(lower=1) + 1)\n",
    "]\n",
    "bp = axes[1].boxplot(amount_data, labels=['Legitimate', 'Fraudulent'],\n",
    "                    patch_artist=True, notch=True)\n",
    "bp['boxes'][0].set_facecolor(COLORS['legit'])\n",
    "bp['boxes'][1].set_facecolor(COLORS['fraud'])\n",
    "axes[1].set_title('Amount Distribution â€” Box Plot Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Logâ‚â‚€(Amount in â‚¦)', fontsize=12)\n",
    "axes[1].set_yticks(tick_values)\n",
    "axes[1].set_yticklabels(tick_labels)\n",
    "\n",
    "plt.suptitle('ğŸ” Fraud vs Legitimate Transaction Amounts in Nigerian Naira (â‚¦)',\n",
    "             fontsize=13, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“Š AMOUNT STATISTICS (Nigerian Naira â‚¦)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                    Legitimate          Fraudulent\n",
    "  Median Amount  :  â‚¦{legit_amounts.median():>12,.0f}    â‚¦{fraud_amounts.median():>12,.0f}\n",
    "  Mean Amount    :  â‚¦{legit_amounts.mean():>12,.0f}    â‚¦{fraud_amounts.mean():>12,.0f}\n",
    "  Max Amount     :  â‚¦{legit_amounts.max():>12,.0f}    â‚¦{fraud_amounts.max():>12,.0f}\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ğŸ’¡ Fraud transactions tend to be larger on average â€” \n",
    "   fraudsters maximize their take per successful attempt.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.3 â€” TIME-BASED FRAUD ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "legit_hour_p = np.array([1,1,1,1,1,2,3,6,7,8,7,7,7,7,7,6,5,5,4,4,3,3,2,1], dtype=float)\n",
    "legit_hour_p /= legit_hour_p.sum()\n",
    "fraud_hour_p = np.array([5,6,7,7,6,5,4,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5], dtype=float)\n",
    "fraud_hour_p /= fraud_hour_p.sum()\n",
    "\n",
    "# --- Fraud rate by hour of day ---\n",
    "hourly = df_raw.groupby('transaction_hour')['is_fraud'].agg(['mean', 'count'])\n",
    "ax = axes[0, 0]\n",
    "bars = ax.bar(hourly.index, hourly['mean'] * 100,\n",
    "              color=[COLORS['fraud'] if h >= 22 or h <= 5 else COLORS['neutral'] \n",
    "                     for h in hourly.index])\n",
    "ax.set_title('ğŸ• Fraud Rate by Hour of Day', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Hour (24-hour format)', fontsize=11)\n",
    "ax.set_ylabel('Fraud Rate (%)', fontsize=11)\n",
    "ax.axvspan(-0.5, 5.5, alpha=0.1, color='red', label='High-risk: 00:00â€“06:00')\n",
    "ax.axvspan(21.5, 23.5, alpha=0.1, color='red')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xticks(range(0, 24, 2))\n",
    "ax.set_xticklabels([f'{h:02d}:00' for h in range(0, 24, 2)], rotation=30)\n",
    "\n",
    "# --- Fraud rate by transaction type ---\n",
    "type_fraud = df_raw.groupby('transaction_type')['is_fraud'].mean().sort_values(ascending=True)\n",
    "ax = axes[0, 1]\n",
    "bars = ax.barh(type_fraud.index, type_fraud.values * 100,\n",
    "               color=[COLORS['fraud'] if v > type_fraud.mean() else COLORS['neutral'] \n",
    "                      for v in type_fraud.values])\n",
    "ax.set_title('ğŸ’³ Fraud Rate by Transaction Type', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Fraud Rate (%)', fontsize=11)\n",
    "ax.axvline(type_fraud.mean() * 100, color='red', linestyle='--', alpha=0.7, label='Average')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# --- Fraud rate by channel ---\n",
    "channel_fraud = df_raw.groupby('channel')['is_fraud'].mean().sort_values(ascending=False)\n",
    "ax = axes[1, 0]\n",
    "colors = [COLORS['fraud'] if v > channel_fraud.mean() else COLORS['legit'] \n",
    "          for v in channel_fraud.values]\n",
    "ax.bar(channel_fraud.index, channel_fraud.values * 100, color=colors, edgecolor='white', linewidth=1.5)\n",
    "ax.set_title('ğŸ“± Fraud Rate by Banking Channel', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Fraud Rate (%)', fontsize=11)\n",
    "ax.set_xticklabels(channel_fraud.index, rotation=25, ha='right')\n",
    "ax.axhline(channel_fraud.mean() * 100, color='gray', linestyle='--', alpha=0.7, label='Average')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# --- Fraud volume by state ---\n",
    "state_fraud = df_raw[df_raw['is_fraud'] == 1].groupby('state').size().sort_values(ascending=True)\n",
    "ax = axes[1, 1]\n",
    "ax.barh(state_fraud.index, state_fraud.values, color=COLORS['fraud'], alpha=0.8, edgecolor='white')\n",
    "ax.set_title('ğŸ—ºï¸  Fraud Transaction Count by State', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Number of Fraud Transactions', fontsize=11)\n",
    "\n",
    "plt.suptitle('ğŸ” Fraud Pattern Analysis â€” Nigerian Banking Transactions',\n",
    "             fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.4 â€” CORRELATION HEATMAP\n",
    "# ============================================================\n",
    "\n",
    "numeric_cols = df_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr_matrix = df_raw[numeric_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "            vmin=-1, vmax=1, center=0, linewidths=0.5, ax=ax,\n",
    "            annot_kws={'size': 9})\n",
    "ax.set_title('Feature Correlation Matrix\\n(Lower Triangle â€” Symmetric Matrix)',\n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with target\n",
    "fraud_corr = corr_matrix['is_fraud'].drop('is_fraud').sort_values(key=abs, ascending=False)\n",
    "print('ğŸ“Š FEATURE CORRELATION WITH FRAUD LABEL (is_fraud):')\n",
    "print('â”€' * 45)\n",
    "for feat, corr_val in fraud_corr.items():\n",
    "    bar = 'â–ˆ' * int(abs(corr_val) * 30)\n",
    "    direction = 'â†‘' if corr_val > 0 else 'â†“'\n",
    "    print(f'  {direction} {feat:<28} {corr_val:+.4f}  {bar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 4: Data Cleaning & Feature Engineering\n",
    "---\n",
    "\n",
    "**Data Quality Issues to address:**\n",
    "1. Missing values in `merchant_category`, `distance_from_home_km`, `avg_balance_ngn`\n",
    "2. Skewed numeric features (log transformation)\n",
    "3. Categorical encoding for ML models\n",
    "4. Feature engineering: extract new signals from existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.1 â€” MISSING VALUE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "missing = df_raw.isnull().sum()\n",
    "missing_pct = (missing / len(df_raw) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing[missing > 0],\n",
    "    'Missing %'    : missing_pct[missing > 0],\n",
    "    'Strategy'     : ['Mode imputation', 'Median imputation', 'Median imputation']\n",
    "})\n",
    "\n",
    "print('ğŸ” MISSING VALUE REPORT:')\n",
    "print('=' * 60)\n",
    "print(missing_df.to_string())\n",
    "print('=' * 60)\n",
    "print(f'\\n  Total missing cells: {missing.sum():,}')\n",
    "print(f'  Overall missing  %: {missing.sum() / df_raw.size * 100:.2f}%')\n",
    "\n",
    "# Visualize missing values\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "missing_plot = missing_pct[missing_pct > 0]\n",
    "bars = ax.barh(missing_plot.index, missing_plot.values,\n",
    "               color=COLORS['accent'], edgecolor='white', linewidth=1.5)\n",
    "for bar, val in zip(bars, missing_plot.values):\n",
    "    ax.text(bar.get_width() + 0.05, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.2f}%', va='center', fontsize=11)\n",
    "ax.set_title('Missing Values by Feature', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Missing Percentage (%)', fontsize=11)\n",
    "ax.set_xlim(0, 6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.2 â€” DATA CLEANING\n",
    "# ============================================================\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "print('ğŸ§¹ CLEANING STEPS:')\n",
    "print('â”€' * 50)\n",
    "\n",
    "# Step 1: Impute missing values\n",
    "df['merchant_category'] = df['merchant_category'].fillna(df['merchant_category'].mode()[0])\n",
    "df['distance_from_home_km'] = df['distance_from_home_km'].fillna(df['distance_from_home_km'].median())\n",
    "df['avg_balance_ngn'] = df['avg_balance_ngn'].fillna(df['avg_balance_ngn'].median())\n",
    "print('  âœ… Step 1: Missing values imputed')\n",
    "\n",
    "# Step 2: Remove impossible values\n",
    "before = len(df)\n",
    "df = df[df['amount_ngn'] > 0]\n",
    "df = df[df['account_age_days'] >= 0]\n",
    "df = df[df['txn_count_last_30d'] >= 0]\n",
    "print(f'  âœ… Step 2: Removed {before - len(df)} invalid rows')\n",
    "\n",
    "# Step 3: Cap extreme outliers (Winsorization at 99.5th percentile)\n",
    "amount_cap = df['amount_ngn'].quantile(0.995)\n",
    "balance_cap = df['avg_balance_ngn'].quantile(0.995)\n",
    "df['amount_ngn'] = df['amount_ngn'].clip(upper=amount_cap)\n",
    "df['avg_balance_ngn'] = df['avg_balance_ngn'].clip(upper=balance_cap)\n",
    "print(f'  âœ… Step 3: Outliers capped â€” Amount ceiling: â‚¦{amount_cap:,.0f}')\n",
    "\n",
    "# Step 4: Extract datetime features\n",
    "df['day_of_week']    = pd.to_datetime(df['transaction_date']).dt.dayofweek\n",
    "df['is_weekend']     = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_night']       = ((df['transaction_hour'] >= 22) | (df['transaction_hour'] <= 5)).astype(int)\n",
    "df['is_work_hours']  = ((df['transaction_hour'] >= 8) & (df['transaction_hour'] <= 17)).astype(int)\n",
    "print('  âœ… Step 4: Datetime features extracted (day_of_week, is_weekend, is_night)')\n",
    "\n",
    "# Step 5: Derived ratio features (domain-specific signals)\n",
    "df['amount_to_balance_ratio'] = df['amount_ngn'] / (df['avg_balance_ngn'] + 1)\n",
    "df['txn_velocity_ratio']      = df['txn_count_last_7d'] / (df['txn_count_last_30d'] / 4 + 1)\n",
    "df['is_large_transaction']    = (df['amount_ngn'] > df['amount_ngn'].quantile(0.90)).astype(int)\n",
    "print('  âœ… Step 5: Derived features created (amount_to_balance_ratio, txn_velocity_ratio)')\n",
    "\n",
    "# Step 6: Log-transform skewed features\n",
    "df['log_amount']           = np.log1p(df['amount_ngn'])\n",
    "df['log_balance']          = np.log1p(df['avg_balance_ngn'])\n",
    "df['log_distance']         = np.log1p(df['distance_from_home_km'])\n",
    "df['log_account_age']      = np.log1p(df['account_age_days'])\n",
    "print('  âœ… Step 6: Log transformation applied to skewed features')\n",
    "\n",
    "print('\\nğŸ“Š CLEANED DATASET:')\n",
    "print(f'  Shape: {df.shape}')\n",
    "print(f'  Missing values: {df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.3 â€” FEATURE ENGINEERING VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "new_features = [\n",
    "    ('amount_to_balance_ratio', 'Amount-to-Balance Ratio'),\n",
    "    ('txn_velocity_ratio', 'Transaction Velocity Ratio'),\n",
    "    ('log_distance', 'Log(Distance from Home)')\n",
    "]\n",
    "\n",
    "for ax, (feat, title) in zip(axes, new_features):\n",
    "    fraud_vals  = df[df['is_fraud'] == 1][feat].clip(upper=df[feat].quantile(0.98))\n",
    "    legit_vals  = df[df['is_fraud'] == 0][feat].clip(upper=df[feat].quantile(0.98))\n",
    "    ax.hist(legit_vals, bins=50, alpha=0.7, color=COLORS['legit'],\n",
    "            label='Legitimate', density=True)\n",
    "    ax.hist(fraud_vals, bins=50, alpha=0.7, color=COLORS['fraud'],\n",
    "            label='Fraudulent', density=True)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "\n",
    "plt.suptitle('ğŸ”§ Engineered Features â€” Separability Between Fraud & Legitimate',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ’¡ FEATURE ENGINEERING RATIONALE:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  amount_to_balance_ratio  â†’ If someone with â‚¦5K balance tries\n",
    "                             to send â‚¦200K â€” high fraud signal\n",
    "  \n",
    "  txn_velocity_ratio       â†’ A sudden spike in weekly activity\n",
    "                             vs. monthly baseline = account takeover\n",
    "  \n",
    "  log_distance             â†’ Transactions 500km from home base\n",
    "                             are unusual (card cloning / travel fraud)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 5: Anomaly Detection\n",
    "### Statistical Approaches to Flag Suspicious Transactions (No Labels Needed)\n",
    "---\n",
    "\n",
    "Anomaly detection is a **rule-based + statistical approach** useful when:\n",
    "- Labels are not available (unsupervised)\n",
    "- You want explainable rules for the compliance/fraud team\n",
    "- As a pre-filter before passing to ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5.1 â€” Z-SCORE ANOMALY DETECTION\n",
    "# ============================================================\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Z-score on log-transformed amount\n",
    "df['zscore_log_amount']   = np.abs(stats.zscore(df['log_amount']))\n",
    "df['zscore_log_balance']  = np.abs(stats.zscore(df['log_balance']))\n",
    "df['zscore_velocity']     = np.abs(stats.zscore(df['txn_velocity_ratio']))\n",
    "df['zscore_distance']     = np.abs(stats.zscore(df['log_distance']))\n",
    "\n",
    "# Flag anomalies (Z > 3 means 3 standard deviations from the mean)\n",
    "Z_THRESHOLD = 3.0\n",
    "df['anomaly_amount']   = (df['zscore_log_amount']   > Z_THRESHOLD).astype(int)\n",
    "df['anomaly_balance']  = (df['zscore_log_balance']  > Z_THRESHOLD).astype(int)\n",
    "df['anomaly_velocity'] = (df['zscore_velocity']     > Z_THRESHOLD).astype(int)\n",
    "df['anomaly_distance'] = (df['zscore_distance']     > Z_THRESHOLD).astype(int)\n",
    "\n",
    "# Composite anomaly score (0â€“4)\n",
    "df['anomaly_score'] = (df['anomaly_amount'] + df['anomaly_balance'] +\n",
    "                       df['anomaly_velocity'] + df['anomaly_distance'])\n",
    "\n",
    "print('ğŸ“Š Z-SCORE ANOMALY DETECTION RESULTS:')\n",
    "print('=' * 55)\n",
    "for signal in ['anomaly_amount', 'anomaly_balance', 'anomaly_velocity', 'anomaly_distance']:\n",
    "    flagged = df[signal].sum()\n",
    "    fraud_caught = df[df[signal] == 1]['is_fraud'].sum()\n",
    "    precision = fraud_caught / flagged if flagged > 0 else 0\n",
    "    recall    = fraud_caught / df['is_fraud'].sum()\n",
    "    print(f'  {signal:<26} | Flagged: {flagged:5,} | '\n",
    "          f'Fraud caught: {fraud_caught:4,} | Precision: {precision:.1%} | Recall: {recall:.1%}')\n",
    "print('=' * 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5.2 â€” RULE-BASED FRAUD SCORING (Interpretable for Business)\n",
    "# ============================================================\n",
    "\n",
    "def rule_based_fraud_score(row):\n",
    "    \"\"\"\n",
    "    Domain-driven fraud scoring system.\n",
    "    Each rule adds points â€” total score indicates fraud risk level.\n",
    "    Rules are inspired by actual Nigerian bank fraud patterns.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    reasons = []\n",
    "    \n",
    "    # Rule 1: Midnight transactions\n",
    "    if row['is_night'] == 1:\n",
    "        score += 2\n",
    "        reasons.append('NIGHT_TXN')\n",
    "    \n",
    "    # Rule 2: Transaction much larger than balance\n",
    "    if row['amount_to_balance_ratio'] > 2.0:\n",
    "        score += 3\n",
    "        reasons.append('EXCEEDS_BALANCE')\n",
    "    \n",
    "    # Rule 3: New recipient + large amount\n",
    "    if row['is_new_recipient'] == 1 and row['amount_ngn'] > 50_000:\n",
    "        score += 3\n",
    "        reasons.append('NEW_RECIPIENT_LARGE')\n",
    "    \n",
    "    # Rule 4: Multiple failed attempts recently\n",
    "    if row['failed_attempts_24h'] >= 2:\n",
    "        score += 3\n",
    "        reasons.append('MULTIPLE_FAILS')\n",
    "    \n",
    "    # Rule 5: Very far from home base\n",
    "    if row['distance_from_home_km'] > 200:\n",
    "        score += 2\n",
    "        reasons.append('FAR_FROM_HOME')\n",
    "    \n",
    "    # Rule 6: Very new account + large transaction\n",
    "    if row['account_age_days'] < 30 and row['amount_ngn'] > 100_000:\n",
    "        score += 4\n",
    "        reasons.append('NEW_ACCOUNT_LARGE_TXN')\n",
    "    \n",
    "    # Rule 7: Unusual activity spike\n",
    "    if row['txn_velocity_ratio'] > 3.0:\n",
    "        score += 2\n",
    "        reasons.append('VELOCITY_SPIKE')\n",
    "    \n",
    "    return score, '|'.join(reasons) if reasons else 'NONE'\n",
    "\n",
    "print('â³ Calculating rule-based fraud scores...')\n",
    "results = df.apply(rule_based_fraud_score, axis=1)\n",
    "df['rule_score']   = results.apply(lambda x: x[0])\n",
    "df['rule_reasons'] = results.apply(lambda x: x[1])\n",
    "\n",
    "# Risk categories\n",
    "def categorize_risk(score):\n",
    "    if score == 0:   return 'LOW'\n",
    "    elif score <= 3: return 'MEDIUM'\n",
    "    elif score <= 6: return 'HIGH'\n",
    "    else:            return 'CRITICAL'\n",
    "\n",
    "df['risk_level'] = df['rule_score'].apply(categorize_risk)\n",
    "\n",
    "print('\\nğŸ“Š RULE-BASED RISK LEVEL DISTRIBUTION:')\n",
    "print('â”€' * 60)\n",
    "for level in ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']:\n",
    "    subset = df[df['risk_level'] == level]\n",
    "    fraud_rate_here = subset['is_fraud'].mean() if len(subset) > 0 else 0\n",
    "    print(f'  {level:<10} | Transactions: {len(subset):7,} | '\n",
    "          f'Fraud Rate: {fraud_rate_here:.1%} | '\n",
    "          f'Actual Fraud: {subset[\"is_fraud\"].sum():5,}')\n",
    "print('â”€' * 60)\n",
    "\n",
    "# Plot risk distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "risk_order = ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']\n",
    "risk_colors = ['#2ECC71', '#F39C12', '#E67E22', '#E74C3C']\n",
    "\n",
    "risk_counts = df['risk_level'].value_counts().reindex(risk_order, fill_value=0)\n",
    "axes[0].bar(risk_counts.index, risk_counts.values, color=risk_colors, edgecolor='white', linewidth=2)\n",
    "axes[0].set_title('Transaction Volume by Risk Level', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Transaction Count', fontsize=11)\n",
    "\n",
    "risk_fraud_rates = df.groupby('risk_level')['is_fraud'].mean().reindex(risk_order, fill_value=0)\n",
    "axes[1].bar(risk_fraud_rates.index, risk_fraud_rates.values * 100,\n",
    "            color=risk_colors, edgecolor='white', linewidth=2)\n",
    "axes[1].set_title('Fraud Rate by Risk Level (%)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Fraud Rate (%)', fontsize=11)\n",
    "\n",
    "plt.suptitle('âš¡ Rule-Based Fraud Scoring â€” Risk Level Analysis',\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 6: Machine Learning Fraud Classification\n",
    "---\n",
    "\n",
    "We train and compare **4 ML models** â€” escalating in complexity:\n",
    "\n",
    "| Model | Complexity | Interpretability | Speed |\n",
    "|-------|-----------|-----------------|-------|\n",
    "| Logistic Regression | Low | â­â­â­â­â­ | Very Fast |\n",
    "| Decision Tree | Low-Medium | â­â­â­â­ | Fast |\n",
    "| Random Forest | High | â­â­â­ | Medium |\n",
    "| Gradient Boosting | Very High | â­â­ | Slow |\n",
    "\n",
    "> **Evaluation Metrics**: For fraud detection, **Recall** (catching as much fraud as possible) and **ROC-AUC** are more important than Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.1 â€” PREPARE FEATURES FOR ML\n",
    "# ============================================================\n",
    "\n",
    "# Select features for the model\n",
    "FEATURE_COLS = [\n",
    "    # Original transaction features\n",
    "    'transaction_hour', 'is_weekend', 'is_night', 'is_work_hours',\n",
    "    'channel', 'transaction_type',\n",
    "    # Amount & balance\n",
    "    'log_amount', 'log_balance', 'amount_to_balance_ratio', 'is_large_transaction',\n",
    "    # Account behavior\n",
    "    'log_account_age', 'txn_count_last_30d', 'txn_count_last_7d', 'txn_velocity_ratio',\n",
    "    # Risk indicators\n",
    "    'log_distance', 'failed_attempts_24h', 'is_new_recipient',\n",
    "    # Anomaly signals\n",
    "    'anomaly_score', 'rule_score'\n",
    "]\n",
    "\n",
    "TARGET_COL = 'is_fraud'\n",
    "\n",
    "# Encode categorical features\n",
    "le_channel = LabelEncoder()\n",
    "le_txn     = LabelEncoder()\n",
    "df['channel']          = le_channel.fit_transform(df['channel'])\n",
    "df['transaction_type'] = le_txn.fit_transform(df['transaction_type'])\n",
    "\n",
    "X = df[FEATURE_COLS].copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "\n",
    "# Train / Validation / Test split (60% / 20% / 20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler  = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_val_sc   = scaler.transform(X_val)\n",
    "X_test_sc  = scaler.transform(X_test)\n",
    "\n",
    "print('âœ… FEATURE PREPARATION COMPLETE')\n",
    "print(f'   Features selected : {len(FEATURE_COLS)}')\n",
    "print(f'   Training set       : {X_train.shape[0]:,} rows ({y_train.mean()*100:.1f}% fraud)')\n",
    "print(f'   Validation set     : {X_val.shape[0]:,} rows ({y_val.mean()*100:.1f}% fraud)')\n",
    "print(f'   Test set           : {X_test.shape[0]:,} rows ({y_test.mean()*100:.1f}% fraud)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.2 â€” TRAIN ALL MODELS\n",
    "# ============================================================\n",
    "\n",
    "print('ğŸ¤– TRAINING MODELS...')\n",
    "print('â”€' * 40)\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression' : LogisticRegression(\n",
    "        class_weight='balanced', max_iter=500, random_state=42, C=0.1\n",
    "    ),\n",
    "    'Decision Tree'        : DecisionTreeClassifier(\n",
    "        class_weight='balanced', max_depth=8, random_state=42,\n",
    "        min_samples_leaf=50\n",
    "    ),\n",
    "    'Random Forest'        : RandomForestClassifier(\n",
    "        class_weight='balanced', n_estimators=100, max_depth=10,\n",
    "        random_state=42, n_jobs=-1, min_samples_leaf=20\n",
    "    ),\n",
    "    'Gradient Boosting'    : GradientBoostingClassifier(\n",
    "        n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "        random_state=42, subsample=0.8\n",
    "    )\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f'  â³ Training {name}...', end='')\n",
    "    model.fit(X_train_sc, y_train)\n",
    "    trained_models[name] = model\n",
    "    print(f' âœ…')\n",
    "\n",
    "print('\\nğŸ‰ All models trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.3 â€” MODEL EVALUATION & COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_model(name, model, X, y, dataset_name='Test'):\n",
    "    \"\"\"Comprehensive model evaluation with business-relevant metrics.\"\"\"\n",
    "    y_pred      = model.predict(X)\n",
    "    y_proba     = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    precision = precision_score(y, y_pred, zero_division=0)\n",
    "    recall    = recall_score(y, y_pred)\n",
    "    f1        = f1_score(y, y_pred)\n",
    "    roc_auc   = roc_auc_score(y, y_proba)\n",
    "    avg_prec  = average_precision_score(y, y_proba)\n",
    "    \n",
    "    return {\n",
    "        'Model'    : name,\n",
    "        'Precision': precision,\n",
    "        'Recall'   : recall,\n",
    "        'F1-Score' : f1,\n",
    "        'ROC-AUC'  : roc_auc,\n",
    "        'Avg Prec' : avg_prec,\n",
    "        'y_pred'   : y_pred,\n",
    "        'y_proba'  : y_proba\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "for name, model in trained_models.items():\n",
    "    results[name] = evaluate_model(name, model, X_test_sc, y_test)\n",
    "\n",
    "# Summary table\n",
    "metrics_df = pd.DataFrame([\n",
    "    {k: v for k, v in res.items() if k not in ['y_pred', 'y_proba']}\n",
    "    for res in results.values()\n",
    "]).set_index('Model').round(4)\n",
    "\n",
    "print('ğŸ“Š MODEL COMPARISON â€” TEST SET PERFORMANCE')\n",
    "print('=' * 75)\n",
    "print(metrics_df.to_string())\n",
    "print('=' * 75)\n",
    "\n",
    "best_model_name = metrics_df['ROC-AUC'].idxmax()\n",
    "print(f'\\n  ğŸ† Best Model (by ROC-AUC): {best_model_name} ({metrics_df.loc[best_model_name, \"ROC-AUC\"]:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.4 â€” CONFUSION MATRICES FOR ALL MODELS\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, result['y_pred'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt=',', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Predicted: Legit', 'Predicted: Fraud'],\n",
    "                yticklabels=['Actual: Legit', 'Actual: Fraud'],\n",
    "                annot_kws={'size': 14})\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    axes[idx].set_title(\n",
    "        f'{name}\\n'\n",
    "        f'ROC-AUC: {result[\"ROC-AUC\"]:.4f} | F1: {result[\"F1-Score\"]:.4f}\\n'\n",
    "        f'TP: {tp:,}  FN: {fn:,}  FP: {fp:,}  TN: {tn:,}',\n",
    "        fontsize=10, fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.suptitle('Confusion Matrices â€” All Models on Test Set',\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“– CONFUSION MATRIX GLOSSARY:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  TP (True Positive)   â†’ Fraud correctly flagged  âœ… (GOAL)\n",
    "  TN (True Negative)   â†’ Legit correctly approved âœ…\n",
    "  FP (False Positive)  â†’ Legit wrongly blocked    âš ï¸ (Bad UX)\n",
    "  FN (False Negative)  â†’ Fraud that slipped through âŒ (Costly!)\n",
    "\n",
    "  In fraud detection: Minimizing FN = catch more fraud\n",
    "  But too many FP = angry customers who can't transact!\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.5 â€” ROC CURVES & PRECISION-RECALL CURVES\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "model_colors = ['#3498DB', '#E67E22', '#2ECC71', '#9B59B6']\n",
    "\n",
    "# --- ROC Curves ---\n",
    "ax = axes[0]\n",
    "for (name, result), color in zip(results.items(), model_colors):\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['y_proba'])\n",
    "    ax.plot(fpr, tpr, color=color, linewidth=2.5,\n",
    "            label=f'{name} (AUC={result[\"ROC-AUC\"]:.4f})')\n",
    "\n",
    "ax.plot([0,1],[0,1], 'k--', alpha=0.4, linewidth=1.5, label='Random (AUC=0.50)')\n",
    "ax.fill_between([0,1],[0,1], alpha=0.05, color='gray')\n",
    "ax.set_title('ROC Curves â€” All Models', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('False Positive Rate (FPR)', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "ax.legend(fontsize=10, loc='lower right')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1.02])\n",
    "\n",
    "# --- Precision-Recall Curves ---\n",
    "ax = axes[1]\n",
    "baseline = y_test.mean()\n",
    "for (name, result), color in zip(results.items(), model_colors):\n",
    "    prec, rec, _ = precision_recall_curve(y_test, result['y_proba'])\n",
    "    avg_p = result['Avg Prec']\n",
    "    ax.plot(rec, prec, color=color, linewidth=2.5,\n",
    "            label=f'{name} (AP={avg_p:.4f})')\n",
    "\n",
    "ax.axhline(y=baseline, color='k', linestyle='--', alpha=0.5, linewidth=1.5,\n",
    "           label=f'Baseline fraud rate ({baseline:.1%})')\n",
    "ax.set_title('Precisionâ€“Recall Curves', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Recall (Fraud Caught)', fontsize=12)\n",
    "ax.set_ylabel('Precision (When we flag, how often right?)', fontsize=12)\n",
    "ax.legend(fontsize=10, loc='upper right')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1.02])\n",
    "\n",
    "plt.suptitle('ğŸ“ˆ Model Performance Curves â€” Fraud Detection',\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.6 â€” FEATURE IMPORTANCE (Best Model)\n",
    "# ============================================================\n",
    "\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "else:\n",
    "    # For Logistic Regression â€” use coefficient magnitudes\n",
    "    importances = np.abs(best_model.coef_[0])\n",
    "    importances = importances / importances.sum()\n",
    "\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': FEATURE_COLS,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "# Color code by feature category\n",
    "def get_color(feat):\n",
    "    if 'anomaly' in feat or 'rule' in feat: return '#9B59B6'\n",
    "    if 'amount' in feat or 'balance' in feat: return '#E74C3C'\n",
    "    if 'distance' in feat or 'night' in feat or 'hour' in feat: return '#F39C12'\n",
    "    if 'velocity' in feat or 'count' in feat: return '#3498DB'\n",
    "    return '#2ECC71'\n",
    "\n",
    "colors = [get_color(f) for f in feat_imp_df['Feature']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "bars = ax.barh(feat_imp_df['Feature'], feat_imp_df['Importance'],\n",
    "               color=colors, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "for bar, val in zip(bars, feat_imp_df['Importance']):\n",
    "    ax.text(bar.get_width() + 0.002, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.4f}', va='center', fontsize=9)\n",
    "\n",
    "ax.set_title(f'Feature Importances â€” {best_model_name}',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Importance Score', fontsize=12)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#9B59B6', label='Anomaly/Rule Scores'),\n",
    "    Patch(facecolor='#E74C3C', label='Amount/Balance'),\n",
    "    Patch(facecolor='#F39C12', label='Time/Location'),\n",
    "    Patch(facecolor='#3498DB', label='Transaction Activity'),\n",
    "    Patch(facecolor='#2ECC71', label='Other'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, fontsize=10, loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nğŸ”‘ TOP 5 MOST IMPORTANT FEATURES ({best_model_name}):')\n",
    "top5 = feat_imp_df.nlargest(5, 'Importance')\n",
    "for _, row in top5.iterrows():\n",
    "    print(f'   {row[\"Feature\"]:<30} â†’ {row[\"Importance\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 7: Business Impact Analysis\n",
    "### Translating Model Performance into Naira (â‚¦) Savings\n",
    "---\n",
    "\n",
    "> *\"The purpose of a fraud model is not accuracy â€” it is profitability and customer protection.\"*\n",
    "\n",
    "This section answers: **\"How much money does this model actually save the bank?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7.1 â€” COST-BENEFIT ANALYSIS FRAMEWORK\n",
    "# ============================================================\n",
    "\n",
    "# Business assumptions (calibrated to Nigerian banking context)\n",
    "ASSUMPTIONS = {\n",
    "    'avg_fraud_loss_ngn'     : 185_000,    # Average amount lost per fraud txn\n",
    "    'investigation_cost_ngn' : 5_000,      # Cost to manually investigate a flagged txn\n",
    "    'customer_churn_cost_ngn': 25_000,     # Revenue lost when legit customer is wrongly blocked\n",
    "    'churn_rate_if_blocked'  : 0.15,       # 15% of blocked legit customers churn\n",
    "    'monthly_txn_volume'     : 1_000_000,  # Bank processes 1M txns/month\n",
    "}\n",
    "\n",
    "print('ğŸ’¼ BUSINESS IMPACT FRAMEWORK')\n",
    "print('=' * 60)\n",
    "print('  Assumptions (Nigerian Banking Context):')\n",
    "for k, v in ASSUMPTIONS.items():\n",
    "    if 'ngn' in k:\n",
    "        print(f'    {k:<35}: â‚¦{v:>12,}')\n",
    "    else:\n",
    "        print(f'    {k:<35}: {v}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7.2 â€” COMPUTE FINANCIAL IMPACT PER MODEL\n",
    "# ============================================================\n",
    "\n",
    "def compute_business_impact(name, result, y_true, monthly_scale=20):\n",
    "    \"\"\"Calculate monthly financial impact of deploying a fraud model.\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_true, result['y_pred'])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Scale to monthly volume\n",
    "    scale = monthly_scale\n",
    "    tp_m = tp * scale\n",
    "    fp_m = fp * scale\n",
    "    fn_m = fn * scale\n",
    "    tn_m = tn * scale\n",
    "    \n",
    "    A = ASSUMPTIONS\n",
    "    \n",
    "    # Financial calculations\n",
    "    fraud_prevented        = tp_m * A['avg_fraud_loss_ngn']\n",
    "    fraud_missed           = fn_m * A['avg_fraud_loss_ngn']\n",
    "    investigation_costs    = (tp_m + fp_m) * A['investigation_cost_ngn']\n",
    "    customer_churn_loss    = fp_m * A['churn_rate_if_blocked'] * A['customer_churn_cost_ngn']\n",
    "    \n",
    "    # Baseline: no model (catch nothing)\n",
    "    total_fraud_without_model = (tp_m + fn_m) * A['avg_fraud_loss_ngn']\n",
    "    \n",
    "    # Net benefit vs. no model\n",
    "    net_monthly_benefit = fraud_prevented - fraud_missed - investigation_costs - customer_churn_loss\n",
    "    net_vs_no_model     = fraud_prevented - investigation_costs - customer_churn_loss\n",
    "    \n",
    "    return {\n",
    "        'Model'                   : name,\n",
    "        'Fraud Txns Caught (TP)'  : tp_m,\n",
    "        'Fraud Missed (FN)'       : fn_m,\n",
    "        'Legit Blocked (FP)'      : fp_m,\n",
    "        'Fraud Prevented â‚¦'      : fraud_prevented,\n",
    "        'Fraud Missed â‚¦'         : fraud_missed,\n",
    "        'Investigation Cost â‚¦'   : investigation_costs,\n",
    "        'Churn Loss â‚¦'           : customer_churn_loss,\n",
    "        'Net Savings vs No Model' : net_vs_no_model,\n",
    "        'ROI %'                   : net_vs_no_model / investigation_costs * 100\n",
    "                                    if investigation_costs > 0 else 0\n",
    "    }\n",
    "\n",
    "impact_results = []\n",
    "for name, result in results.items():\n",
    "    impact = compute_business_impact(name, result, y_test, monthly_scale=20)\n",
    "    impact_results.append(impact)\n",
    "\n",
    "impact_df = pd.DataFrame(impact_results).set_index('Model')\n",
    "\n",
    "# Format for display\n",
    "print('\\nğŸ’° MONTHLY FINANCIAL IMPACT PER MODEL')\n",
    "print('=' * 90)\n",
    "display_cols = ['Fraud Prevented â‚¦', 'Fraud Missed â‚¦', 'Investigation Cost â‚¦',\n",
    "                'Churn Loss â‚¦', 'Net Savings vs No Model', 'ROI %']\n",
    "\n",
    "for model_name, row in impact_df[display_cols].iterrows():\n",
    "    print(f'\\n  ğŸ“Š {model_name}')\n",
    "    for col, val in row.items():\n",
    "        if 'â‚¦' in col:\n",
    "            print(f'     {col:<30} : â‚¦{val:>15,.0f}')\n",
    "        elif 'ROI' in col:\n",
    "            print(f'     {col:<30} : {val:>14.1f}%')\n",
    "print('=' * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7.3 â€” BUSINESS IMPACT VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# --- Stacked cost/benefit bars ---\n",
    "ax = axes[0]\n",
    "model_names = list(impact_df.index)\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "fraud_prev_billions = impact_df['Fraud Prevented â‚¦'] / 1e9\n",
    "total_costs_billions = (impact_df['Investigation Cost â‚¦'] + impact_df['Churn Loss â‚¦']) / 1e9\n",
    "\n",
    "bars1 = ax.bar(x - width/2, fraud_prev_billions, width, label='Fraud Prevented',\n",
    "               color=COLORS['legit'], edgecolor='white', linewidth=2)\n",
    "bars2 = ax.bar(x + width/2, total_costs_billions, width, label='Total Costs',\n",
    "               color=COLORS['fraud'], edgecolor='white', linewidth=2)\n",
    "\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'â‚¦{bar.get_height():.1f}B', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'â‚¦{bar.get_height():.2f}B', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_title('Monthly Benefit vs Cost (â‚¦ Billions)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Amount (â‚¦ Billions)', fontsize=11)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.replace(' ', '\\n') for m in model_names], fontsize=9)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "# --- Net savings comparison ---\n",
    "ax = axes[1]\n",
    "net_billions = impact_df['Net Savings vs No Model'] / 1e9\n",
    "bar_colors = [COLORS['legit'] if v > 0 else COLORS['fraud'] for v in net_billions]\n",
    "bars = ax.bar(model_names, net_billions, color=bar_colors, edgecolor='white', linewidth=2)\n",
    "for bar, val in zip(bars, net_billions):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2,\n",
    "            bar.get_height() + (0.02 if val >= 0 else -0.05),\n",
    "            f'â‚¦{val:.2f}B', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.axhline(0, color='black', linewidth=1.5, linestyle='--')\n",
    "ax.set_title('Net Monthly Savings vs No-Fraud-Detection (â‚¦ Billions)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Net Savings (â‚¦ Billions)', fontsize=11)\n",
    "ax.set_xticklabels([m.replace(' ', '\\n') for m in model_names], fontsize=9)\n",
    "\n",
    "plt.suptitle('ğŸ’° Business Impact Analysis â€” Monthly Financial Value of Fraud Detection Models',\n",
    "             fontsize=13, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 8: Threshold Optimization\n",
    "### Finding the Optimal Decision Threshold for the Best Model\n",
    "---\n",
    "\n",
    "By default, ML models classify fraud when probability > 0.5. But we can tune this threshold to:\n",
    "- **Lower threshold** â†’ Catch more fraud (higher Recall) but flag more legit transactions (more FP)\n",
    "- **Higher threshold** â†’ Fewer false alarms but miss more fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8.1 â€” THRESHOLD OPTIMIZATION\n",
    "# ============================================================\n",
    "\n",
    "best_model_result = results[best_model_name]\n",
    "y_proba_best      = best_model_result['y_proba']\n",
    "\n",
    "thresholds  = np.arange(0.05, 0.95, 0.01)\n",
    "precision_t = []\n",
    "recall_t    = []\n",
    "f1_t        = []\n",
    "net_save_t  = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_pred_t = (y_proba_best >= thr).astype(int)\n",
    "    p = precision_score(y_test, y_pred_t, zero_division=0)\n",
    "    r = recall_score(y_test, y_pred_t, zero_division=0)\n",
    "    f = f1_score(y_test, y_pred_t, zero_division=0)\n",
    "    \n",
    "    # Net savings at this threshold\n",
    "    cm_t = confusion_matrix(y_test, y_pred_t)\n",
    "    tn_t, fp_t, fn_t, tp_t = cm_t.ravel()\n",
    "    scale = 20\n",
    "    net = ((tp_t * scale * ASSUMPTIONS['avg_fraud_loss_ngn'])\n",
    "         - ((tp_t + fp_t) * scale * ASSUMPTIONS['investigation_cost_ngn'])\n",
    "         - (fp_t * scale * ASSUMPTIONS['churn_rate_if_blocked']\n",
    "            * ASSUMPTIONS['customer_churn_cost_ngn'])) / 1e9\n",
    "    \n",
    "    precision_t.append(p)\n",
    "    recall_t.append(r)\n",
    "    f1_t.append(f)\n",
    "    net_save_t.append(net)\n",
    "\n",
    "optimal_idx  = np.argmax(f1_t)\n",
    "optimal_thr  = thresholds[optimal_idx]\n",
    "best_net_idx = np.argmax(net_save_t)\n",
    "best_net_thr = thresholds[best_net_idx]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(thresholds, precision_t, color='#3498DB', linewidth=2.5, label='Precision')\n",
    "ax.plot(thresholds, recall_t,    color='#E74C3C', linewidth=2.5, label='Recall')\n",
    "ax.plot(thresholds, f1_t,        color='#2ECC71', linewidth=2.5, label='F1-Score')\n",
    "ax.axvline(optimal_thr, color='#F39C12', linestyle='--', linewidth=2,\n",
    "           label=f'Best F1 Threshold = {optimal_thr:.2f}')\n",
    "ax.axvline(0.50, color='gray', linestyle=':', linewidth=1.5, label='Default = 0.50')\n",
    "ax.set_title('Metrics vs Decision Threshold', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Threshold', fontsize=11)\n",
    "ax.set_ylabel('Score', fontsize=11)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(thresholds, net_save_t, color='#9B59B6', linewidth=3)\n",
    "ax.fill_between(thresholds, net_save_t, 0,\n",
    "                where=np.array(net_save_t) > 0, alpha=0.2, color='#2ECC71')\n",
    "ax.axvline(best_net_thr, color='#F39C12', linestyle='--', linewidth=2,\n",
    "           label=f'Max Profit Threshold = {best_net_thr:.2f}')\n",
    "ax.axvline(0.50, color='gray', linestyle=':', linewidth=1.5, label='Default = 0.50')\n",
    "ax.set_title('Monthly Net Savings (â‚¦B) vs Decision Threshold', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Threshold', fontsize=11)\n",
    "ax.set_ylabel('Monthly Net Savings (â‚¦ Billions)', fontsize=11)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.suptitle(f'âš™ï¸  Threshold Optimization â€” {best_model_name}',\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“Œ THRESHOLD RECOMMENDATIONS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Default threshold (0.50):\n",
    "    F1-Score: {f1_t[49]:.4f} | Net Savings: â‚¦{net_save_t[49]:.2f}B/month\n",
    "\n",
    "  Optimal for F1 ({optimal_thr:.2f}):\n",
    "    F1-Score: {f1_t[optimal_idx]:.4f} | Net Savings: â‚¦{net_save_t[optimal_idx]:.2f}B/month\n",
    "\n",
    "  Optimal for profit ({best_net_thr:.2f}):\n",
    "    F1-Score: {f1_t[best_net_idx]:.4f} | Net Savings: â‚¦{net_save_t[best_net_idx]:.2f}B/month\n",
    "\n",
    "  ğŸ’¡ RECOMMENDATION: Use threshold {best_net_thr:.2f} for production\n",
    "     deployment to maximize financial return.\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 9: Executive Summary & Conclusions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.1 â€” FINAL SUMMARY DASHBOARD\n",
    "# ============================================================\n",
    "\n",
    "best_result = results[best_model_name]\n",
    "best_impact = [r for r in impact_results if r['Model'] == best_model_name][0]\n",
    "\n",
    "print(\"\"\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘    NIGERIAN BANK FRAUD DETECTION â€” EXECUTIVE SUMMARY REPORT    â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“‹ PROJECT OVERVIEW\n",
    "  Dataset      : 50,000 simulated Nigerian bank transactions (2023)\n",
    "  Fraud Rate   : 2.5% ({df['is_fraud'].sum():,} fraudulent transactions)\n",
    "  Total â‚¦ at risk (Test): â‚¦{df_raw[df_raw['is_fraud']==1]['amount_ngn'].sum():,.0f}\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ğŸ¤– BEST PERFORMING MODEL: {best_model_name}\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  ROC-AUC Score    : {best_result['ROC-AUC']:.4f}  (1.0 = perfect, 0.5 = random)\n",
    "  Precision        : {best_result['Precision']:.4f}  (When we flag fraud, we're correct {best_result['Precision']*100:.1f}% of the time)\n",
    "  Recall           : {best_result['Recall']:.4f}  (We catch {best_result['Recall']*100:.1f}% of all actual fraud)\n",
    "  F1-Score         : {best_result['F1-Score']:.4f}  (Harmonic mean of Precision & Recall)\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ğŸ’° MONTHLY FINANCIAL IMPACT (Scaled to 1M transactions/month)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Fraud Transactions Caught  : {best_impact['Fraud Txns Caught (TP)']:>10,.0f}\n",
    "  Fraud Transactions Missed  : {best_impact['Fraud Missed (FN)']:>10,.0f}\n",
    "  Legitimate Transactions Blocked (FP): {best_impact['Legit Blocked (FP)']:>5,.0f}\n",
    "\n",
    "  Fraud Losses Prevented     : â‚¦{best_impact['Fraud Prevented â‚¦']:>15,.0f}\n",
    "  Investigation Costs        : â‚¦{best_impact['Investigation Cost â‚¦']:>15,.0f}\n",
    "  Customer Churn Loss        : â‚¦{best_impact['Churn Loss â‚¦']:>15,.0f}\n",
    "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  NET MONTHLY SAVINGS        : â‚¦{best_impact['Net Savings vs No Model']:>15,.0f}\n",
    "  ANNUAL SAVINGS             : â‚¦{best_impact['Net Savings vs No Model']*12:>15,.0f}\n",
    "  ROI                        : {best_impact['ROI %']:>14.1f}%\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ğŸ¯ KEY FRAUD SIGNALS DISCOVERED (Top 5 Predictors)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\"\"\")\n",
    "\n",
    "for i, (_, row) in enumerate(top5.iterrows(), 1):\n",
    "    print(f\"  {i}. {row['Feature']:<32} â†’ Importance: {row['Importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âš ï¸  FRAUD RISK PATTERNS IDENTIFIED\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  ğŸŒ™ Late Night (22:00â€“06:00): Higher fraud rate than daytime\n",
    "  ğŸ’¸ Large Amounts vs Balance: Key indicator of account takeover\n",
    "  ğŸƒ Transaction Velocity:    Unusual spikes = credential stuffing\n",
    "  ğŸŒ Distance from Home:      Geolocation anomalies flag card fraud\n",
    "  ğŸ†• New Accounts + Big Txns: Common money mule pattern in Nigeria\n",
    "  ğŸ”„ New Recipients:          80% of fraud goes to new accounts\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ğŸš€ DEPLOYMENT RECOMMENDATIONS\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  1. PRODUCTION THRESHOLD: Use {best_net_thr:.2f} (maximizes profit vs. 0.50 default)\n",
    "  2. REAL-TIME SCORING: Integrate with NIBSS for sub-100ms decisions\n",
    "  3. MONITORING: Track model drift monthly â€” fraud patterns evolve\n",
    "  4. HUMAN REVIEW: Route HIGH/CRITICAL risk transactions to fraud team\n",
    "  5. FEEDBACK LOOP: Retrain quarterly with confirmed fraud labels\n",
    "  6. EXPLAINABILITY: Use rule reasons to communicate blocks to customers\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.2 â€” FINAL COMPREHENSIVE DASHBOARD\n",
    "# ============================================================\n",
    "\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "fig.suptitle('ğŸ¦ Nigerian Bank Fraud Detection â€” Final Dashboard',\n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "# Model comparison bar chart\n",
    "ax1 = fig.add_subplot(3, 4, (1, 2))\n",
    "metric_names = ['Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "x_m = np.arange(len(metric_names))\n",
    "bar_width = 0.18\n",
    "model_colors_list = ['#3498DB', '#E67E22', '#2ECC71', '#9B59B6']\n",
    "for i, (mname, mresult) in enumerate(results.items()):\n",
    "    vals = [mresult['Precision'], mresult['Recall'], mresult['F1-Score'], mresult['ROC-AUC']]\n",
    "    ax1.bar(x_m + i * bar_width, vals, bar_width,\n",
    "            label=mname, color=model_colors_list[i], alpha=0.85, edgecolor='white')\n",
    "ax1.set_xticks(x_m + bar_width * 1.5)\n",
    "ax1.set_xticklabels(metric_names, fontsize=10)\n",
    "ax1.set_title('Model Performance Comparison', fontsize=11, fontweight='bold')\n",
    "ax1.legend(fontsize=8, loc='lower right')\n",
    "ax1.set_ylim(0, 1.15)\n",
    "ax1.axhline(1.0, color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "# ROC curve (best model)\n",
    "ax2 = fig.add_subplot(3, 4, (3, 4))\n",
    "fpr_b, tpr_b, _ = roc_curve(y_test, best_result['y_proba'])\n",
    "ax2.plot(fpr_b, tpr_b, color='#E74C3C', linewidth=3,\n",
    "         label=f'{best_model_name}\\nAUC={best_result[\"ROC-AUC\"]:.4f}')\n",
    "ax2.plot([0,1],[0,1], 'k--', alpha=0.4)\n",
    "ax2.fill_between(fpr_b, tpr_b, alpha=0.15, color='#E74C3C')\n",
    "ax2.set_title(f'ROC Curve â€” {best_model_name}', fontsize=11, fontweight='bold')\n",
    "ax2.set_xlabel('FPR', fontsize=10)\n",
    "ax2.set_ylabel('TPR', fontsize=10)\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "# Confusion matrix (best model)\n",
    "ax3 = fig.add_subplot(3, 4, 5)\n",
    "cm_best = confusion_matrix(y_test, best_result['y_pred'])\n",
    "sns.heatmap(cm_best, annot=True, fmt=',', cmap='Blues', ax=ax3,\n",
    "            xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'],\n",
    "            annot_kws={'size': 11})\n",
    "ax3.set_title(f'Confusion Matrix\\n{best_model_name}', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Feature importance (top 10)\n",
    "ax4 = fig.add_subplot(3, 4, (6, 7))\n",
    "top10 = feat_imp_df.tail(10)\n",
    "ax4.barh(top10['Feature'], top10['Importance'],\n",
    "         color=[get_color(f) for f in top10['Feature']], edgecolor='white')\n",
    "ax4.set_title('Top 10 Feature Importances', fontsize=10, fontweight='bold')\n",
    "ax4.set_xlabel('Importance', fontsize=9)\n",
    "\n",
    "# Risk level fraud rates\n",
    "ax5 = fig.add_subplot(3, 4, 8)\n",
    "risk_rates = df.groupby('risk_level')['is_fraud'].mean().reindex(risk_order, fill_value=0) * 100\n",
    "ax5.bar(risk_rates.index, risk_rates.values, color=['#2ECC71','#F39C12','#E67E22','#E74C3C'],\n",
    "        edgecolor='white', linewidth=1.5)\n",
    "ax5.set_title('Fraud Rate by Risk Level', fontsize=10, fontweight='bold')\n",
    "ax5.set_ylabel('Fraud %', fontsize=9)\n",
    "\n",
    "# Fraud by hour\n",
    "ax6 = fig.add_subplot(3, 4, (9, 10))\n",
    "hourly_rate = df.groupby('transaction_hour')['is_fraud'].mean() * 100\n",
    "ax6.fill_between(hourly_rate.index, hourly_rate.values, alpha=0.6, color='#E74C3C')\n",
    "ax6.plot(hourly_rate.index, hourly_rate.values, color='#E74C3C', linewidth=2)\n",
    "ax6.axvspan(-0.5, 5.5, alpha=0.1, color='red')\n",
    "ax6.axvspan(21.5, 23.5, alpha=0.1, color='red')\n",
    "ax6.set_title('Fraud Rate by Hour of Day', fontsize=10, fontweight='bold')\n",
    "ax6.set_xlabel('Hour', fontsize=9)\n",
    "ax6.set_ylabel('Fraud %', fontsize=9)\n",
    "ax6.set_xticks(range(0, 24, 3))\n",
    "\n",
    "# Monthly savings comparison\n",
    "ax7 = fig.add_subplot(3, 4, (11, 12))\n",
    "net_savings = [r['Net Savings vs No Model'] / 1e9 for r in impact_results]\n",
    "model_labels = [r['Model'].replace(' ', '\\n') for r in impact_results]\n",
    "bar_c = [COLORS['legit'] if v > 0 else COLORS['fraud'] for v in net_savings]\n",
    "ax7.bar(model_labels, net_savings, color=bar_c, edgecolor='white', linewidth=1.5)\n",
    "ax7.set_title('Monthly Net Savings (â‚¦B)', fontsize=10, fontweight='bold')\n",
    "ax7.set_ylabel('â‚¦ Billions/month', fontsize=9)\n",
    "ax7.axhline(0, color='black', linewidth=1.2)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig('/home/claude/fraud_detection_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Dashboard saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“š Skills Demonstrated in This Project\n",
    "\n",
    "| Category | Skills |\n",
    "|----------|--------|\n",
    "| **Data Engineering** | Simulating realistic domain data, handling missing values, feature engineering, Winsorization, log transformation |\n",
    "| **EDA** | Distribution analysis, correlation matrices, temporal pattern analysis, segment comparison |\n",
    "| **Anomaly Detection** | Z-score anomaly detection, rule-based fraud scoring, composite risk scoring |\n",
    "| **Machine Learning** | Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, Stratified Train/Test splits, class imbalance handling |\n",
    "| **Model Evaluation** | ROC-AUC, Precision-Recall, Confusion Matrix, F1-Score, threshold optimization |\n",
    "| **Business Impact** | Cost-benefit analysis, ROI calculation, financial modeling in Nigerian Naira |\n",
    "| **Communication** | Executive summary, visualization dashboards, business recommendations |\n",
    "| **Domain Knowledge** | NIBSS payment ecosystem, Nigerian banking patterns, FITC fraud statistics |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Next Steps & Extensions\n",
    "\n",
    "1. **XGBoost / LightGBM** â€” More powerful gradient boosting for higher AUC\n",
    "2. **SMOTE Oversampling** â€” Synthetic data generation to address class imbalance\n",
    "3. **Network Analysis** â€” Graph-based fraud ring detection (Neo4j)\n",
    "4. **Real-time API** â€” Flask/FastAPI endpoint for live transaction scoring\n",
    "5. **Explainability** â€” SHAP values for individual transaction explanations\n",
    "6. **Time-Series** â€” Sequential fraud detection with LSTM networks\n",
    "7. **MLOps** â€” Model monitoring, drift detection, automated retraining pipeline\n",
    "\n",
    "---\n",
    "\n",
    "*Built with â¤ï¸ for the Nigerian FinTech ecosystem | Python 3.x | Scikit-learn*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
